"""
gpt-memory-api

A simple REST API for GPT responses using embeddings backed by a Redis database.

Allows users to ingest text data to generate embeddings and store them in a Redis Vector Store, and
provides a interface where the GPT model can answer queries using the stored embeddings for context.

Endpoints:
/ingest   - POST: Ingest text data for embeddings.
/query    - POST: Get a GPT response to a query, using conversation history and embeddings.
/health   - GET:  Health check endpoint to monitor the service.

The application uses aiohttp for the web server and can be run inside a Docker container.
"""

import os
import asyncio
from typing import Optional
from aiohttp import web
import redis
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Redis as RedisVectorStore
from langchain.schema import HumanMessage, SystemMessage, AIMessage
from langchain.docstore.document import Document


class GPTMemory:
    """
    GPTMemory

    A helper class that uses LangChain to generate embeddings and answer queries using GPT models.
    """

    def __init__(
        self,
        redis_socket_path: str,
        chat_temperature: float = 0.6,
        chat_max_tokens: int = 1000,
        chat_uri: Optional[str] = None,
    ):
        """
        Initialize the GPTMemory class.

        Args:
            redis_socket_path (str): The path to the Redis Unix socket.
            openai_api_key (str): Your OpenAI API key.
        """
        self.redis_socket_path = redis_socket_path

        # Initialize embeddings
        self.embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

        # Initialize Chat LLM
        self.llm = ChatOpenAI(
            model="gpt-4o-mini",
            temperature=chat_temperature,
            max_tokens=chat_max_tokens,
            base_url=chat_uri,
        )

        # Initialize Redis client
        self.redis_client = redis.Redis(unix_socket_path=redis_socket_path)

        # Placeholder for vector store
        self.vector_store = None

    def ingest_text(self, text: str, chunk_size: int = 1000, chunk_overlap: int = 200):
        """
        Ingest and process text data.

        Args:
            text (str): The text data to ingest.
            chunk_size (int): The size of each text chunk. Default is 1000.
            chunk_overlap (int): The number of overlapping characters between chunks. Default is 200.
        """
        # Clean and chunk text
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size, chunk_overlap=chunk_overlap
        )
        texts = text_splitter.split_text(text)

        # Create documents
        docs = [Document(page_content=t) for t in texts]

        # Initialize vector store and add documents
        if self.vector_store is None:
            self.vector_store = RedisVectorStore.from_documents(
                documents=docs,
                embedding=self.embeddings,
                redis_client=self.redis_client,
                index_name="embeddings",
            )
        else:
            self.vector_store.add_documents(docs)

    def answer_query_with_history(self, conversation: list) -> str:
        """
        Answer a query using the GPT model, conversation history, and the vector store.

        Args:
            conversation (list): The conversation history.

        Returns:
            str: The answer generated by the GPT model.
        """
        if self.vector_store is None:
            raise ValueError("Vector store is empty. Please ingest some data first.")

        # Extract the last user message
        last_user_message = None
        for message in reversed(conversation):
            if message["role"] == "user":
                last_user_message = message["content"]
                break

        if not last_user_message:
            raise ValueError("No user message found in conversation.")

        # Retrieve relevant documents from the vector store
        retriever = self.vector_store.as_retriever()
        relevant_docs = retriever.get_relevant_documents(last_user_message)

        # Prepare the context from retrieved documents
        context = "\n\n".join([doc.page_content for doc in relevant_docs])

        # Prepare the modified last message
        modified_message = (
            "Below are some historical messages that may help your response. "
            "If they don't have the answer you need, simply use them for style inspiration.\n\n"
            f"{context}\n\n"
            f"Query: {last_user_message}"
        )

        # Reconstruct the conversation with modified last message
        # Convert conversation messages to LangChain message objects
        lc_messages = []
        for msg in conversation[:-1]:
            if msg["role"] == "system":
                lc_messages.append(SystemMessage(content=msg["content"]))
            elif msg["role"] == "user":
                lc_messages.append(HumanMessage(content=msg["content"]))
            elif msg["role"] == "assistant":
                lc_messages.append(AIMessage(content=msg["content"]))

        # Add the modified last message
        lc_messages.append(HumanMessage(content=modified_message))

        # Generate the response using the ChatOpenAI model
        assistant_reply = self.llm.invoke(input=lc_messages).content

        assert isinstance(assistant_reply, str), "The response from the model is not a string"

        return assistant_reply


# AIOHTTP Web Server
async def ingest(request):
    data = await request.json()
    text = data.get("text", "")
    if not text:
        return web.json_response({"error": "No text provided"}, status=400)

    # Ingest the text
    request.app["gpt_memory"].ingest_text(text)
    return web.json_response({"status": "Text ingested successfully"})


async def query(request):
    data = await request.json()
    conversation = data.get("conversation", [])
    if not conversation:
        return web.json_response({"error": "No conversation provided"}, status=400)

    # Process the conversation
    try:
        response = request.app["gpt_memory"].answer_query_with_history(conversation)
        return web.json_response({"response": response})
    except ValueError as e:
        return web.json_response({"error": str(e)}, status=400)


async def health(request):
    return web.json_response({"status": "healthy"}, status=200)


async def create_app():
    app = web.Application()

    # Initialize the GPTMemory helper
    gpt_memory = GPTMemory(
        redis_socket_path=os.environ.get("REDIS_SOCKET_PATH", "/redis_socket/redis.sock"),
    )

    app["gpt_memory"] = gpt_memory

    # Add routes
    app.add_routes([
        web.post("/ingest", ingest),
        web.post("/query", query),
        web.get("/health", health),
    ])

    return app


if __name__ == "__main__":
    import uvicorn

    app = asyncio.run(create_app())
    uvicorn.run(app, host="0.0.0.0", port=8080)
